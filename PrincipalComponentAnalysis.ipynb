{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Philippe Rigollet. 18.650 Statistics for Applications . Fall 2016. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = (X^1, \\dots , X^d)^T$ be a $d$-dimensional random vector and $X_1, \\dots, X_n$ be $n$ independent copies of $X$.\n",
    "\n",
    "We write $X_i = (X_i^1, \\dots, X_i^d)^T$ for $i=1, \\dots, n$.\n",
    "\n",
    "Mean of $X$ is given by\n",
    "\n",
    "$$\\mathbb{E}[X] = (\\mathbb{E}[X^1], \\dots, \\mathbb{E}[X^d])^T.$$\n",
    "\n",
    "Covarince matrix of $X$ is given by $\\Sigma = (\\sigma_{jk})_{j,k=1,\\dots,d} $ where \n",
    "\n",
    "$$\\sigma_{jk} = \\text{cov}(X^j, X^k).$$\n",
    "\n",
    "One shows that \n",
    "\n",
    "$$\\Sigma = \\mathbb{E}[XX^T] - \\mathbb{E}[X]\\mathbb{E}^T = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^T].$$\n",
    "\n",
    "Empirical mean of $X_1, \\dots, X_n$ is given by\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i = (\\bar{X}^1, \\dots, \\bar{X}^d)^T.$$\n",
    "\n",
    "Empirical covariance of $X_1, \\dots, X_n$ is given by the matrix $S = (s_{jk})_{j,k=1,\\dots,d}$, where $s_{jk}$ is the empirical covariance of $X_i^j, X_i^k, i=1,\\dots,n$.\n",
    "\n",
    "One shows that \n",
    "\n",
    "$$S= \\frac{1}{n} \\sum_{i=1}^{n} X_i X_i^T - \\bar{X}\\bar{X}^T  = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})(X_i - \\bar{X})^T.$$\n",
    "\n",
    "$\\Sigma$ and $S$ are symmetric, positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition** If $u \\in \\mathbb{R}^d$,\n",
    "\n",
    "- $u^T \\Sigma u$ is the variance of $u^T X$\n",
    "\n",
    "- $u^T S u$ is the sample variance of $u^T X_1, \\dots, u^T X_n$.\n",
    "\n",
    "In particular, $u^T Su$ measures how spread the points are in direction of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reason for PCA**\n",
    "\n",
    "The sample $X_1, \\dots, X_n$ makes a cloud of points in $\\mathbb{R}^d$.\n",
    "\n",
    "In practice, $d$ is large. If $d > 3$, it becomes impossible to represent the cloud on a picture.\n",
    "\n",
    "**Question:** Is it possible to project the cloud onto a linear subspace of dimension $d' < d$ by keeping as much information as possible?\n",
    "\n",
    "**Answer:** PCA does this by keeping as much covariance structre as possible by keeping orthogonal directions that discriminate well the points of the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Algorithm**\n",
    "\n",
    "1. Input: $X_1, \\dots, X_n$: cloud of $n$ points in dimension $d$.\n",
    "\n",
    "2. Compute the empirical covariance matrix.\n",
    "\n",
    "3. Compute the decomposition $S = PDP^T$, where $D = \\text{Diag}(\\lambda_1, \\dots, \\lambda_d)$ with $\\lambda_1 \\geq \\cdots  \\geq \\lambda_d \\geq 0 $ and $P = (v_1, \\dots, v_d)$ is an orthgonal matrix.\n",
    "\n",
    "4. Choose $k < d$ and set $P_k = (v_1, \\dots, v_k) \\in \\mathbb{R}^{d \\times k}.$\n",
    "\n",
    "5. Output: Principal components $Y_1, \\dots, Y_n$, where\n",
    "\n",
    "$$Y_i  = P_k^T X_i \\in \\mathbb{R}^k, \\ i=1, \\dots, n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Main Principle**\n",
    "\n",
    "We find the collection of orthogonal directions in which the cloud is much spread out.\n",
    "\n",
    "$$v_1 \\in \\text{argmax}_{\\| u \\| = 1} u^T S u,$$\n",
    "\n",
    "$$v_2 \\in \\text{argmax}_{\\| u \\| = 1, u ⊥ v_1} u^T S u,$$\n",
    "\n",
    "$$ \\cdots $$\n",
    "\n",
    "$$v_d \\in \\text{argmax}_{\\| u \\| = 1, u ⊥ v_j, j=1,\\dots,d-1} u^T S u.$$\n",
    "\n",
    "Hence, the $k$ orthogonal directions in which the cloud is the most spread out correspond exactly to the eigenvectors associated with $k$ largest values of $S$.\n",
    "\n",
    "$\\lambda_1 + \\cdots + \\lambda_k$ is called the variance explained by the PCA and $\\lambda_1 + \\cdots + \\lambda_d = \\text{Trace}(S)$ is the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
